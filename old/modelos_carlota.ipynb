{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeres proves de models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías que vamos a estar usando en general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>ctx-4</th>\n",
       "      <th>ctx-3</th>\n",
       "      <th>ctx-2</th>\n",
       "      <th>ctx-1</th>\n",
       "      <th>percentage_docks_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>0.403409</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.289773</td>\n",
       "      <td>0.373106</td>\n",
       "      <td>0.304924</td>\n",
       "      <td>0.238636</td>\n",
       "      <td>0.240530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.253788</td>\n",
       "      <td>0.268939</td>\n",
       "      <td>0.350379</td>\n",
       "      <td>0.344697</td>\n",
       "      <td>0.393939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.390152</td>\n",
       "      <td>0.346591</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.186047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.218992</td>\n",
       "      <td>0.437984</td>\n",
       "      <td>0.515504</td>\n",
       "      <td>0.470930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id    year  month  day  hour     ctx-4     ctx-3     ctx-2  \\\n",
       "0         1.0  2020.0    1.0  1.0   2.0  0.522727  0.503788  0.469697   \n",
       "1         1.0  2020.0    1.0  1.0   7.0  0.289773  0.373106  0.304924   \n",
       "2         1.0  2020.0    1.0  1.0  12.0  0.253788  0.268939  0.350379   \n",
       "3         1.0  2020.0    1.0  1.0  17.0  0.390152  0.346591  0.255814   \n",
       "4         1.0  2020.0    1.0  1.0  22.0  0.083333  0.218992  0.437984   \n",
       "\n",
       "      ctx-1  percentage_docks_available  \n",
       "0  0.403409                    0.354167  \n",
       "1  0.238636                    0.240530  \n",
       "2  0.344697                    0.393939  \n",
       "3  0.220930                    0.186047  \n",
       "4  0.515504                    0.470930  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pth = \"/Users/albam/Desktop/UNIVERSITAT/POSTGRAU/CAPSTONE_PROJECT/Gorrapiedra-proyecto/data/formated data\"\n",
    "\n",
    "df_20 = pd.read_csv(os.path.join(pth, \"data_2020.csv\"), index_col=False, skipinitialspace=True, skip_blank_lines=True)\n",
    "df_21 = pd.read_csv(os.path.join(pth, \"data_2021.csv\"), index_col=False, skipinitialspace=True, skip_blank_lines=True)\n",
    "df_22 = pd.read_csv(os.path.join(pth, \"data_2022.csv\"), index_col=False, skipinitialspace=True, skip_blank_lines=True)\n",
    "df_23 = pd.read_csv(os.path.join(pth, \"data_2023.csv\"), index_col=False, skipinitialspace=True, skip_blank_lines=True)\n",
    "df = pd.concat([df_20, df_21, df_22, df_23], ignore_index = True)\n",
    "\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(['Unnamed: 0'], axis=1)   # Eliminar columna Unnamed\n",
    "\n",
    "# Visualitzar el df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el dataset en training i test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['percentage_docks_available']\n",
    "X = df.drop(['percentage_docks_available'], axis=1)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.3, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "cat_cols = ['station_id', 'month', 'day', 'year']\n",
    "x_train_encoded = pd.get_dummies(X_train, columns=cat_cols)\n",
    "x_validation_encoded = pd.get_dummies(X_validation, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns in validation set: {'station_id_529.0', 'station_id_530.0'}\n",
      "Training set shape after deleting columns: (2311772, 567)\n",
      "Validation set shape: (990760, 567)\n"
     ]
    }
   ],
   "source": [
    "# Veiem que el training set y el validation set tenen diferent número de columnes després del encoding. Mirem quines son les columnes que falten al validation set.\n",
    "missing_cols = set(x_train_encoded.columns) - set(x_validation_encoded.columns)\n",
    "print(\"Missing columns in validation set:\", missing_cols)\n",
    "\n",
    "# Si hay columnas que faltan en el validation set, las borramos del training set\n",
    "for col in missing_cols:\n",
    "    x_train_encoded.drop(col, axis=1, inplace=True)\n",
    "\n",
    "print(\"Training set shape after deleting columns:\", x_train_encoded.shape)\n",
    "print(\"Validation set shape:\", x_validation_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (2311772, 567)\n",
      "Validation set shape: (990760, 567)\n"
     ]
    }
   ],
   "source": [
    "# Normalitzar les dades\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_encoded_scal = scaler.fit_transform(x_train_encoded)\n",
    "x_validation_encoded_scal = scaler.fit_transform(x_validation_encoded)\n",
    "\n",
    "print(\"Training set shape:\", x_train_encoded_scal.shape)\n",
    "print(\"Validation set shape:\", x_validation_encoded_scal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.012968499604314544\n",
      "RMSE: 0.11387932035411234\n",
      "MAE: 0.07396116467106605\n",
      "R2: 0.8403306339531335\n"
     ]
    }
   ],
   "source": [
    "# Decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "dtr = DecisionTreeRegressor(max_depth=7)\n",
    "\n",
    "dtr.fit(x_train_encoded_scal, y_train)\n",
    "\n",
    "y_pred = dtr.predict(x_validation_encoded_scal)\n",
    "\n",
    "mse = mean_squared_error(y_validation, y_pred)\n",
    "rmse = mean_squared_error(y_validation, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_validation, y_pred)\n",
    "r2 = r2_score(y_validation, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R2:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.012448674170680326\n",
      "RMSE: 0.11157362668068259\n",
      "MAE: 0.07177192709104638\n",
      "R2: 0.846730772749128\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgbr = XGBRegressor(n_estimators=1000, learning_rate=0.01, n_jobs=4)\n",
    "\n",
    "xgbr.fit(x_train_encoded_scal, y_train)\n",
    "\n",
    "y_pred = xgbr.predict(x_validation_encoded_scal)\n",
    "\n",
    "mse = mean_squared_error(y_validation, y_pred)\n",
    "rmse = mean_squared_error(y_validation, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_validation, y_pred)\n",
    "r2 = r2_score(y_validation, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m36122/36122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 6ms/step - loss: 0.0231 - root_mean_squared_error: 0.1479 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1181\n",
      "Epoch 2/10\n",
      "\u001b[1m36122/36122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 5ms/step - loss: 0.0150 - root_mean_squared_error: 0.1224 - val_loss: 0.0139 - val_root_mean_squared_error: 0.1177\n",
      "Epoch 3/10\n",
      "\u001b[1m36122/36122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 6ms/step - loss: 0.0148 - root_mean_squared_error: 0.1216 - val_loss: 0.0133 - val_root_mean_squared_error: 0.1151\n",
      "Epoch 4/10\n",
      "\u001b[1m36122/36122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 5ms/step - loss: 0.0147 - root_mean_squared_error: 0.1211 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1173\n",
      "Epoch 5/10\n",
      "\u001b[1m36122/36122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 7ms/step - loss: 0.0147 - root_mean_squared_error: 0.1211 - val_loss: 0.0144 - val_root_mean_squared_error: 0.1201\n",
      "Epoch 6/10\n",
      "\u001b[1m36122/36122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 7ms/step - loss: 0.0145 - root_mean_squared_error: 0.1206 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1168\n",
      "Epoch 7/10\n",
      "\u001b[1m36122/36122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 6ms/step - loss: 0.0145 - root_mean_squared_error: 0.1206 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1213\n",
      "Epoch 8/10\n",
      "\u001b[1m36122/36122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 4ms/step - loss: 0.0145 - root_mean_squared_error: 0.1203 - val_loss: 0.0124 - val_root_mean_squared_error: 0.1113\n",
      "Epoch 9/10\n",
      "\u001b[1m36122/36122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 4ms/step - loss: 0.0145 - root_mean_squared_error: 0.1202 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1234\n",
      "Epoch 10/10\n",
      "\u001b[1m36122/36122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 7ms/step - loss: 0.0144 - root_mean_squared_error: 0.1202 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1185\n",
      "\u001b[1m30962/30962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 415us/step - loss: 0.0140 - root_mean_squared_error: 0.1185\n",
      "\u001b[1m30962/30962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 473us/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to list.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     35\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mpredict(x_validation_encoded_scal)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Plot loss\u001b[39;00m\n\u001b[1;32m     41\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSprop, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_f\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNhid1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNhid2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNhid3\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Bs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Final loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to list.__format__"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "lr = 1e-3\n",
    "Nhid1 = 64\n",
    "Nhid2 = 64\n",
    "Nhid3 = 32\n",
    "activ = 'relu'\n",
    "loss_f = 'mse'\n",
    "bs = 64\n",
    "epochs = 10\n",
    "\n",
    "# Define the neural network architecture\n",
    "net = keras.Sequential([\n",
    "    layers.Input(shape=(x_train_encoded_scal.shape[1],)), \n",
    "    layers.Dense(Nhid1, activation=activ),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(Nhid2, activation=activ),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(Nhid3, activation=activ),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "net.compile(optimizer=keras.optimizers.RMSprop(learning_rate=lr), loss=loss_f, metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Train the model\n",
    "history = net.fit(x_train_encoded_scal, y_train, epochs=epochs, batch_size=bs, validation_data=(x_validation_encoded_scal, y_validation))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = net.evaluate(x_validation_encoded_scal, y_validation, verbose=1)  \n",
    "\n",
    "# Make predictions\n",
    "y_pred = net.predict(x_validation_encoded_scal)\n",
    "\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "\n",
    "# Plot loss\n",
    "\n",
    "title = f\"RMSprop, {loss_f}, {lr}, {Nhid1}, {Nhid2}, {Nhid3}, Bs {bs}, Epoch: {epochs}, Final loss: {loss:.4f}\"\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title(title)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(f'../outputs/loss-ann/{title}_loss.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_validation, y_pred, c='crimson')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "p1 = max(max(y_pred), max(y_validation))\n",
    "p2 = min(min(y_pred), min(y_validation))\n",
    "plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "plt.xlabel('True Values', fontsize=15)\n",
    "plt.ylabel('Predictions', fontsize=15)\n",
    "plt.axis('equal')\n",
    "plt.savefig(f'../outputs/loss-ann/{title}_pred.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network \n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, Nin, Nhid1, Nout):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Define linear transformations and activation functions\n",
    "        self.actfun = nn.LeakyReLU()\n",
    "\n",
    "        self.lc1 = nn.Linear(Nin, Nhid1, bias=True)\n",
    "\n",
    "        self.lc2 = nn.Linear(Nhid1, Nout, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.lc2(self.actfun(self.lc1(x)))\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Hiperparàmetres generals de la xarxa\n",
    "\n",
    "Nin = x_train_encoded_scal.shape[1] # nombre d'inputs\n",
    "Nout = 1 # nombre d'outputs\n",
    "lr = 0.1 #learning rate\n",
    "epochs = 30 # nombre d'iteracions\n",
    "\n",
    "# Parametres de la xarxa    \n",
    "Nhid1 = 5\n",
    "\n",
    "# Creem la xarxa\n",
    "model = NeuralNetwork(Nin=Nin, Nhid1=Nhid1,  Nout=Nout)\n",
    "\n",
    "# Optimitzador\n",
    "optimizer = torch.optim.Rprop(params=model.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.Adam(params=model.to(device).parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "#loss_fn = nn.SmoothL1Loss() # L1 loss\n",
    "\n",
    "net = model.to(device) # instanciem l'objecte\n",
    "\n",
    "print('NN architecture: \\n', net)\n",
    "print(\"Layers and parameters:\\n\")\n",
    "for name, param in net.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:50]} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(f\"x_train_encoded_scal shape: {x_train_encoded_scal.shape}, dtype: {x_train_encoded_scal.dtype}\")\n",
    "print(f\"y_train shape: {y_train.shape}, dtype: {y_train.dtype}\")\n",
    "print(f\"x_validation_encoded_scal shape: {x_validation_encoded_scal.shape}, dtype: {x_validation_encoded_scal.dtype}\")\n",
    "print(f\"y_validation shape: {y_validation.shape}, dtype: {y_validation.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import psutil\n",
    "\n",
    "def get_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    total_memory = mem.total / (1024 ** 3)  # Convert bytes to GB\n",
    "    available_memory = mem.available / (1024 ** 3)  # Convert bytes to GB\n",
    "    used_memory = total_memory - available_memory\n",
    "    percent_used = (used_memory / total_memory) * 100\n",
    "    return available_memory, used_memory, percent_used\n",
    "\n",
    "available_memory, used_memory, percent_used = get_memory_usage()\n",
    "print(f\"Available memory: {available_memory:.2f} GB\")\n",
    "print(f\"Used memory: {used_memory:.2f} GB\")\n",
    "print(f\"Percentage used: {percent_used:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_list_tr = []\n",
    "loss_list_val = []\n",
    "print(\"hello\")\n",
    "\n",
    "try:\n",
    "    print(\"Converting and moving training data...\")\n",
    "    train_set = torch.tensor(x_train_encoded_scal.reshape(-1,1), dtype=torch.float32).detach().to(device)\n",
    "    print(\"Train set loaded successfully\")\n",
    "    \n",
    "    target = torch.tensor(y_train, dtype=torch.float32).squeeze().detach().to(device)\n",
    "    print(\"Target loaded successfully\")\n",
    "    \n",
    "    validation_set = torch.tensor(x_validation_encoded_scal.reshape(-1,1), dtype=torch.float32).detach().to(device)\n",
    "    print(\"Validation set loaded successfully\")\n",
    "    \n",
    "    validation = torch.tensor(y_validation, dtype=torch.float32).squeeze().detach().to(device)\n",
    "    print(\"Validation target loaded successfully\")\n",
    "except RuntimeError as e:\n",
    "    print(\"RuntimeError:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "target_loader_train = DataLoader(target, batch_size=batch_size, shuffle=True)\n",
    "target_loader_val = DataLoader(validation, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r_training = trange(epochs, desc=\"Training\", leave=True, colour=\"blue\")\n",
    "for t in r_training:\n",
    "    print(t)\n",
    "\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "    pred = net(train_set).squeeze()  # predicció\n",
    "    loss = loss_fn(pred, target)  # càlcul del cost\n",
    "    loss_list_tr.append(loss.item())\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val = net(validation_set).squeeze()\n",
    "    loss_v = loss_fn(val, validation)  # càlcul del cost\n",
    "    loss_list_val.append(loss_v.item())\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Plots\n",
    "    if t%10 == 0 or t == epochs - 1: # or (t + 1) % 200 == 0 or t == epochs - 1:\n",
    "        plt.plot(loss_list_tr, label='Training loss')\n",
    "        plt.plot(loss_list_val, label='Validation loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    r_training.set_postfix_str(\"L1: %6.4f\" % (loss.item()))\n",
    "\n",
    "r_training.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
