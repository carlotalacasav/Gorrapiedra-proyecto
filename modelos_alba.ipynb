{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeres proves de models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías que vamos a estar usando en general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3275160, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pth = \"/Users/albam/Desktop/UNIVERSITAT/POSTGRAU/CAPSTONE_PROJECT/Gorrapiedra-proyecto/data/formated data\"\n",
    "info_stations = '/Users/albam/Desktop/UNIVERSITAT/POSTGRAU/CAPSTONE_PROJECT/Gorrapiedra-proyecto/data/info bicing/Informacio_Estacions_Bicing.csv'\n",
    "\n",
    "df_20 = pd.read_csv(os.path.join(pth, \"data_2020.csv\"), index_col=False, skipinitialspace=True, skip_blank_lines=True)\n",
    "df_21 = pd.read_csv(os.path.join(pth, \"data_2021.csv\"), index_col=False, skipinitialspace=True, skip_blank_lines=True)\n",
    "df_22 = pd.read_csv(os.path.join(pth, \"data_2022.csv\"), index_col=False, skipinitialspace=True, skip_blank_lines=True)\n",
    "df_23 = pd.read_csv(os.path.join(pth, \"data_2023.csv\"), index_col=False, skipinitialspace=True, skip_blank_lines=True)\n",
    "df_info = pd.read_csv(os.path.join(info_stations), index_col=False, skipinitialspace=True, skip_blank_lines=True)\n",
    "\n",
    "dfs_list = [df_20, df_21, df_22, df_23, df_info]\n",
    "\n",
    "# Eliminem les estacions que no estan a tots els datasets\n",
    "\n",
    "for proto_df in dfs_list:       # fyi: en aquest cas, totes les estacions estan a tots els datasets\n",
    "    for otro_df in dfs_list:\n",
    "        if proto_df is not otro_df:\n",
    "            proto_df = proto_df[proto_df['station_id'].isin(otro_df['station_id'])]\n",
    "\n",
    "df_info_stations = df_info[['station_id', 'lat', 'lon', 'post_code', 'capacity']]\n",
    "\n",
    "df = pd.concat([df_20, df_21, df_22, df_23], ignore_index = True)\n",
    "df = pd.merge(df, df_info_stations, on='station_id')\n",
    "\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(['Unnamed: 0'], axis=1)   # Eliminar columna Unnamed\n",
    "\n",
    "# Visualitzar el df\n",
    "print(\"Shape of the df: \", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ara que el dataset té més info, s'haurà de canviar el preprocessament potser. Com veieu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el dataset en training i test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['percentage_docks_available']\n",
    "X = df.drop(['percentage_docks_available'], axis=1)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.3, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "cat_cols = ['station_id']#, 'month', 'day', 'year']\n",
    "x_train_encoded = pd.get_dummies(X_train, columns=cat_cols)\n",
    "x_validation_encoded = pd.get_dummies(X_validation, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veiem que el training set y el validation set tenen diferent número de columnes després del encoding. Mirem quines son les columnes que falten al validation set.\n",
    "missing_cols = set(x_train_encoded.columns) - set(x_validation_encoded.columns)\n",
    "print(\"Missing columns in validation set:\", missing_cols)\n",
    "\n",
    "# Si hay columnas que faltan en el validation set, las borramos del training set\n",
    "for col in missing_cols:\n",
    "    x_train_encoded.drop(col, axis=1, inplace=True)\n",
    "\n",
    "print(\"Training set shape after deleting columns:\", x_train_encoded.shape)\n",
    "print(\"Validation set shape:\", x_validation_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalitzar les dades\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_encoded_scal = scaler.fit_transform(x_train_encoded)\n",
    "x_validation_encoded_scal = scaler.fit_transform(x_validation_encoded)\n",
    "\n",
    "print(\"Training set shape:\", x_train_encoded_scal.shape)\n",
    "print(\"Validation set shape:\", x_validation_encoded_scal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "dtr = DecisionTreeRegressor(max_depth=7)\n",
    "\n",
    "dtr.fit(x_train_encoded_scal, y_train)\n",
    "\n",
    "y_pred = dtr.predict(x_validation_encoded_scal)\n",
    "\n",
    "mse = mean_squared_error(y_validation, y_pred)\n",
    "rmse = mean_squared_error(y_validation, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_validation, y_pred)\n",
    "r2 = r2_score(y_validation, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R2:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgbr = XGBRegressor(n_estimators=1000, learning_rate=0.01, n_jobs=4)\n",
    "\n",
    "xgbr.fit(x_train_encoded_scal, y_train)\n",
    "\n",
    "y_pred = xgbr.predict(x_validation_encoded_scal)\n",
    "\n",
    "mse = mean_squared_error(y_validation, y_pred)\n",
    "rmse = mean_squared_error(y_validation, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_validation, y_pred)\n",
    "r2 = r2_score(y_validation, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "lr = 1e-2\n",
    "Nhid1 = 64\n",
    "Nhid2 = 64\n",
    "Nhid3 = 32\n",
    "activ = 'relu'\n",
    "loss_f = 'mse'\n",
    "bs = 64\n",
    "epochs = 10\n",
    "\n",
    "# Define the neural network architecture\n",
    "net = keras.Sequential([\n",
    "    layers.Input(shape=(x_train_encoded_scal.shape[1],)), \n",
    "    layers.Dense(Nhid1, activation='tanh'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(Nhid2, activation=activ),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(Nhid3, activation=activ),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "net.compile(optimizer=keras.optimizers.RMSprop(learning_rate=lr), loss=loss_f, metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Train the model\n",
    "history = net.fit(x_train_encoded_scal, y_train, epochs=epochs, batch_size=bs, validation_data=(x_validation_encoded_scal, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = net.evaluate(x_validation_encoded_scal, y_validation, verbose=1)  \n",
    "\n",
    "# Make predictions\n",
    "y_pred = net.predict(x_validation_encoded_scal)\n",
    "\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "\n",
    "# Plot loss\n",
    "\n",
    "title = f\"RMSprop, {loss_f}, {lr}, {Nhid1}, {Nhid2}, {Nhid3}, Bs {bs}, Epoch: {epochs}, Final loss: {loss:.4f}\"\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title(title)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(f'/Users/albam/Desktop/UNIVERSITAT/POSTGRAU/CAPSTONE_PROJECT/Gorrapiedra-proyecto/outputs/loss-ann/{title}_loss.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network \n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, Nin, Nhid1, Nout):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Define linear transformations and activation functions\n",
    "        self.actfun = nn.LeakyReLU()\n",
    "\n",
    "        self.lc1 = nn.Linear(Nin, Nhid1, bias=True)\n",
    "\n",
    "        self.lc2 = nn.Linear(Nhid1, Nout, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.lc2(self.actfun(self.lc1(x)))\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Hiperparàmetres generals de la xarxa\n",
    "\n",
    "Nin = x_train_encoded_scal.shape[1] # nombre d'inputs\n",
    "Nout = 1 # nombre d'outputs\n",
    "lr = 0.1 #learning rate\n",
    "epochs = 30 # nombre d'iteracions\n",
    "\n",
    "# Parametres de la xarxa    \n",
    "Nhid1 = 5\n",
    "\n",
    "# Creem la xarxa\n",
    "model = NeuralNetwork(Nin=Nin, Nhid1=Nhid1,  Nout=Nout)\n",
    "\n",
    "# Optimitzador\n",
    "optimizer = torch.optim.Rprop(params=model.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.Adam(params=model.to(device).parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "#loss_fn = nn.SmoothL1Loss() # L1 loss\n",
    "\n",
    "net = model.to(device) # instanciem l'objecte\n",
    "\n",
    "print('NN architecture: \\n', net)\n",
    "print(\"Layers and parameters:\\n\")\n",
    "for name, param in net.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:50]} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(f\"x_train_encoded_scal shape: {x_train_encoded_scal.shape}, dtype: {x_train_encoded_scal.dtype}\")\n",
    "print(f\"y_train shape: {y_train.shape}, dtype: {y_train.dtype}\")\n",
    "print(f\"x_validation_encoded_scal shape: {x_validation_encoded_scal.shape}, dtype: {x_validation_encoded_scal.dtype}\")\n",
    "print(f\"y_validation shape: {y_validation.shape}, dtype: {y_validation.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import psutil\n",
    "\n",
    "def get_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    total_memory = mem.total / (1024 ** 3)  # Convert bytes to GB\n",
    "    available_memory = mem.available / (1024 ** 3)  # Convert bytes to GB\n",
    "    used_memory = total_memory - available_memory\n",
    "    percent_used = (used_memory / total_memory) * 100\n",
    "    return available_memory, used_memory, percent_used\n",
    "\n",
    "available_memory, used_memory, percent_used = get_memory_usage()\n",
    "print(f\"Available memory: {available_memory:.2f} GB\")\n",
    "print(f\"Used memory: {used_memory:.2f} GB\")\n",
    "print(f\"Percentage used: {percent_used:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_list_tr = []\n",
    "loss_list_val = []\n",
    "print(\"hello\")\n",
    "\n",
    "try:\n",
    "    print(\"Converting and moving training data...\")\n",
    "    train_set = torch.tensor(x_train_encoded_scal.reshape(-1,1), dtype=torch.float32).detach().to(device)\n",
    "    print(\"Train set loaded successfully\")\n",
    "    \n",
    "    target = torch.tensor(y_train, dtype=torch.float32).squeeze().detach().to(device)\n",
    "    print(\"Target loaded successfully\")\n",
    "    \n",
    "    validation_set = torch.tensor(x_validation_encoded_scal.reshape(-1,1), dtype=torch.float32).detach().to(device)\n",
    "    print(\"Validation set loaded successfully\")\n",
    "    \n",
    "    validation = torch.tensor(y_validation, dtype=torch.float32).squeeze().detach().to(device)\n",
    "    print(\"Validation target loaded successfully\")\n",
    "except RuntimeError as e:\n",
    "    print(\"RuntimeError:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "target_loader_train = DataLoader(target, batch_size=batch_size, shuffle=True)\n",
    "target_loader_val = DataLoader(validation, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r_training = trange(epochs, desc=\"Training\", leave=True, colour=\"blue\")\n",
    "for t in r_training:\n",
    "    print(t)\n",
    "\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "    pred = net(train_set).squeeze()  # predicció\n",
    "    loss = loss_fn(pred, target)  # càlcul del cost\n",
    "    loss_list_tr.append(loss.item())\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val = net(validation_set).squeeze()\n",
    "    loss_v = loss_fn(val, validation)  # càlcul del cost\n",
    "    loss_list_val.append(loss_v.item())\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Plots\n",
    "    if t%10 == 0 or t == epochs - 1: # or (t + 1) % 200 == 0 or t == epochs - 1:\n",
    "        plt.plot(loss_list_tr, label='Training loss')\n",
    "        plt.plot(loss_list_val, label='Validation loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    r_training.set_postfix_str(\"L1: %6.4f\" % (loss.item()))\n",
    "\n",
    "r_training.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
